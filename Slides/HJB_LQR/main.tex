\documentclass{beamer}

\input{settings.tex}


\title{Riccati eq., Linear Quadratic Regulator}
\subtitle{Control Theory, Lecture 8}
\author{by Sergei Savin}
\centering
\date{\mydate}



\begin{document}
\maketitle


\begin{frame}{Content}
\begin{itemize}
\item Hamilton-Jacobi-Bellman equation
\item Algebraic Riccati equation
\item Linear Quadratic Regulator (LQR)
\item Discrete LQR
\end{itemize}
\end{frame}

\begin{frame}{Control policy}
%\framesubtitle{Definitions}
\begin{flushleft}

Let us define dynamics:

\begin{equation}
    \dot {\bo{x}} = \bo{f} (\bo{x}, \bo{u})
\end{equation}
%
with initial conditions $\bo{x}(0)=\bo{x}_0$. 

\bigskip

Additionally we define \emph{control policy} as:

\begin{equation}
    \bo{u} = \pi (\bo{x}, t)
\end{equation}

To connect with the previous ways we talked about control, we can say that choosing different control gains and different feed-forward control amounts to choosing a different control policy.

\end{flushleft}
\end{frame}




\begin{frame}{Cost, optimal cost}
%\framesubtitle{}
\begin{flushleft}

Let $J$ be an additive cost function:

\begin{equation}
J (\bo{x}_0, \pi (\bo{x}, t)) = \int_0^\infty g(\bo{x}, \bo{u}) dt
\end{equation}
%
where $g(\bo{x}, \bo{u})$ is instantaneous cost and $\bo{x}_0 = \bo{x}(0)$ is the initial conditions. Notice that $J$ depends on $\bo{x}_0$ rather than $\bo{x}(t)$, since initial conditions and control policy completely define the trajectory of the system $\bo{x}(t)$.


\bigskip

Let $J^*$ be the optimal (lowest possible) cost. In other words:

\begin{equation}
J^*(\bo{x}_0) = \underset{\pi}{\inf{}} J(\bo{x}_0, \pi (\bo{x}, t))
\end{equation}

Optimal cost is attained when optimal policy is attained: $\pi = \pi^*(\bo{x}, t)$

\end{flushleft}
\end{frame}



\begin{frame}{Continuous-time Dynamic Programming Principle (DPP)}
	%\framesubtitle{}
	\begin{flushleft}
		
		Consider the optimal trajectory $\bo{x}^*$ and two time moments $t_1$ and $t_2 > t_1$, where $|t_2 - t_1| \rightarrow 0$. The relation between $J^*(\bo{x}^*(t_1))$ and $J^*(\bo{x}^*(t_2))$ is captured by the following expression:
		%
		\begin{equation}
			J^*(\bo{x}^*(t_1)) = \inf_\pi \left(  \underbrace{\int_{t_1}^{t_2} g(\bo{x}, \bo{u}) dt }_{\Delta J} +  J^*(\bo{x}^*(t_2))  \right)
		\end{equation}
		%
		It implies that $J^*(\bo{x}^*(t_2))$, which is "closer to the goal", is smaller than $J^*(\bo{x}^*(t_1))$ by $\Delta J$; where as $\Delta J$ represents the additional cost that will be included in $J^*(\bo{x}^*(t_1))$, needed to go from $\bo{x}^*(t_1)$ to $\bo{x}^*(t_2)$. This is the \emph{Continuous-time Dynamic Programming Principle (DPP)}.
		
		\bigskip
		
		\textcolor{mygray}{more in the appendix}
		
	\end{flushleft}
\end{frame}



\begin{frame}{DPP -> HJB}
	%\framesubtitle{}
	\begin{flushleft}
		
		We can set $t_1 = 0$ and $t_2 = h$, and expand nonlinear functions $\Delta J$ and $J^*(\bo{x}^*(h))$ in the vicinity of $t = 0$:
		%
		\begin{align}
			\Delta J &= h g(\bo{x}, \bo{u}) + o(h)
			\\
			J^*(\bo{x}^*(h)) &= J^*(\bo{x}^*(0)) + h \frac{\partial J}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u}) + o(h)
		\end{align}
		
		Then DPP takes the form:
		%
		\begin{equation}
			J^*(\bo{x}^*(0)) = \inf_\pi \left(  h g(\bo{x}, \bo{u})  +  J^*(\bo{x}^*(0)) + h \frac{\partial J}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u}) + o(h)  \right)
		\end{equation}
		
		Cancel $J^*(\bo{x}^*(0))$ on both sides, divide by $h$ and take a limit $h \rightarrow 0$:
		%
		\begin{equation}
			0 = \inf_\pi \left( g(\bo{x}, \bo{u})  +  \frac{\partial J}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u})  \right)
		\end{equation}
		
		
	\end{flushleft}
\end{frame}



\begin{frame}{Hamilton-Jacobi-Bellman equation, 1}
% \framesubtitle{HJB}
\begin{flushleft}

With this, we can formulate \emph{Hamilton-Jacobi-Bellman equation} (HJB):

\begin{equation}
\label{eq:HJB_0}
\underset{\bo{u}}{\inf} \ 
\left[ 
g(\bo{x}, \bo{u}) + 
\frac{\partial J^*}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u}) 
\right] = 0
\end{equation}

\bigskip


If the minimizer $\bo{u}$ exists, we can describe it as:

\begin{equation}
\bo{u}^* = \underset{\bo{u}}{\text{argmin}} \ 
\left[ 
g(\bo{x}, \bo{u}) + 
\frac{\partial J^*}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u}) \right] 
\end{equation}

The term $\frac{\partial J^*}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u})$ represents a derivative of $J^*$ with respect to the vector field $\bo{f} (\bo{x}, \bo{u})$. 


\end{flushleft}
\end{frame}





\begin{frame}{Hamilton-Jacobi-Bellman equation, 2}
	% \framesubtitle{HJB}
	\begin{flushleft}
		
		The core idea behind HJB is that for any sub-optimal control law the rate at which you incur cost $g(\bo{x}, \bo{u})$ outpaces the rate $\frac{\partial J^*}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u})$  at which the "optimal cost-to-go from the current position to the goal" decreases: 
		
		\begin{equation}
		g(\bo{x}, \bo{u}) + 
		\frac{\partial J^*}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u}) > 0
		\end{equation}
		
		and only for the optimal control policy the HJB holds:
		
		\begin{equation}
			g(\bo{x}, \bo{u}^*) + 
			\frac{\partial J^*}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u}^*) 
			= 0
		\end{equation}
		
	\end{flushleft}
\end{frame}




\begin{frame}{Algebraic Riccati (LTI case), 1}
%\framesubtitle{HJB for LTI}
\begin{flushleft}

For LTI, dynamics is:
\begin{equation}
\dot {\bo{x}} = \bo{A}  \bo{x} + \bo{B} \bo{u}
\end{equation}

We can choose quadratic cost:
\begin{equation}
g(\bo{x}, \bo{u} ) = 
\bo{x}^\top \bo{Q} \bo{x} +
\bo{u} ^\top \bo{R} \bo{u} 
\end{equation}
%
where $\bo{Q} = \bo{Q}^\top \geq 0 $ is a positive semidefinite matrix and $\bo{R} = \bo{R}^\top > 0$ is a positive-definite matrix.

\bigskip

There is a theorem that says that for LTI with quadratic cost, $J^*$ has the form:

\begin{equation}
	J^* = \bo{x}^\top \bo{S} \bo{x}
\end{equation}
%
where $\bo{S} = \bo{S}^\top \geq 0$.


\end{flushleft}
\end{frame}



\begin{frame}{Algebraic Riccati (LTI case), 2}
%\framesubtitle{HJB for LTI, part 2}
\begin{flushleft}

Let us compute the term $\frac{\partial J^*}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u}) $ that appears in the HJB. Using the fact that $J^* =\bo{x}^\top \bo{S} \bo{x}$ we re-write it as:

\begin{equation}
	\frac{\partial J^*}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u}) = \frac{d}{dt}(\bo{x}^\top \bo{S}\bo{x})
	=
	\dot{\bo{x}}^\top \bo{S}\bo{x} + \bo{x}^\top \bo{S}\dot{\bo{x}} 
\end{equation}

Since $\dot {\bo{x}} = \bo{A}  \bo{x} + \bo{B} \bo{u}$ we can continue the derivation:

\begin{equation}
	... =
	(\bo{A}  \bo{x} + \bo{B} \bo{u})^\top \bo{S}\bo{x} + \bo{x}^\top \bo{S}(\bo{A}  \bo{x} + \bo{B} \bo{u})
\end{equation}


\bigskip

Remembering that $g(\bo{x}, \bo{u} ) = 
\bo{x}^\top \bo{Q} \bo{x} +
\bo{u} ^\top \bo{R} \bo{u}$ we write the HJB $\underset{\bo{u}}{\min} \ 
\left[ 
g(\bo{x}, \bo{u}) + 
\frac{\partial J^*}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u}) 
\right] = 0$ as:

\[
\underset{\bo{u}}{\min} \ 
\left [ 
\mathbf  x^\top \bo{Q} \bo{x} +
\mathbf  u^\top \bo{R} \bo{u}
+ 
\bo{x}^\top \bo{S}
(\bo{A} \bo{x} + \bo{B} \bo{u}) 
+ 
(\bo{A} \bo{x} + \bo{B} \bo{u})^\top
\bo{S} \bo{x}
\right ] = 0
\]

\end{flushleft}
\end{frame}






\begin{frame}{Algebraic Riccati (LTI case), 3}
	%\framesubtitle{HJB for LTI, part 2}
	\begin{flushleft}
		
		We can simplify the expression $\mathbf  x^\top \bo{Q} \bo{x} +
		\bo{u}^\top \bo{R} \bo{u}
		+ 
		\bo{x}^\top \bo{S}
		(\bo{A} \bo{x} + \bo{B} \bo{u}) 
		+ 
		(\bo{A} \bo{x} + \bo{B} \bo{u})^\top
		\bo{S} \bo{x}$ as:


		\[
		\underset{\bo{u}}{\min} \ 
		\left [ 
		\bo{u}^\top \bo{R} \bo{u}
		+ 
		\bo{x}^\top (
		\bo{Q} + \bo{S} \bo{A} + \bo{A}^\top \bo{S}
		)\bo{x}
		+ 
		\bo{x}^\top \bo{S} \bo{B} \bo{u} 
		+ \bo{u}^\top \bo{B}^\top \bo{S} \bo{x} 
		\right ] = 0
		\]
		
		
		\bigskip
		
		The minimum is achieved when the function is at an extremum, meaning $\frac{\partial}{\partial \bo{u}} (...) = 0$.
		
	\end{flushleft}
\end{frame}





\begin{frame}{Linear Quadratic Regulator, 1}
%\framesubtitle{}
\begin{flushleft}


Setting the partial derivatives of $\bo{u}^\top \bo{R} \bo{u}
+ 
\bo{x}^\top (
\bo{Q} + \bo{S} \bo{A} + \bo{A}^\top \bo{S}
)\bo{x}
+ 
\bo{x}^\top \bo{S} \bo{B} \bo{u} 
+ \bo{u}^\top \bo{B}^\top \bo{S} \bo{x}$ to zero:


\begin{align}
2 \bo{u} ^\top \bo{R} + 
2 \bo{x}^\top \bo{S} \bo{B} = 0
\\
\bo{R} \bo{u} + 
 \bo{B}^\top   \bo{S} \bo{x}= 0
 \\
 \boxed{
 \bo{u} = 
 -\bo{R}^{-1} \bo{B}^\top \bo{S} \bo{x}}
\end{align}

This is the desired control law. We can see that it is \emph{proportional}. We can re-write it as:

\begin{equation}
\bo{u} = -\bo{K} \bo{x}
\end{equation}

where $\bo{K}  = \bo{R}^{-1} \bo{B}^\top \bo{S}$ is the controller gain. This control law is called \emph{Linear Quadratic Regulator (LQR)}.

\end{flushleft}
\end{frame}




\begin{frame}{Linear Quadratic Regulator, 2}
	%\framesubtitle{}
	\begin{flushleft}
		
		We substitute $\bo{u} = -\bo{R}^{-1} \bo{B}^\top \bo{S} \bo{x}$ into the Algebraic Riccati eq.  $\bo{u}^\top \bo{R} \bo{u}
		+ 
		\bo{x}^\top (
		\bo{Q} + \bo{S} \bo{A} + \bo{A}^\top \bo{S}
		)\bo{x}
		+ 
		\bo{x}^\top \bo{S} \bo{B} \bo{u} 
		+ \bo{u}^\top \bo{B}^\top \bo{S} \bo{x}$:
		
		\begin{align*}
			\hspace*{-0.7cm}
			\textcolor{myred}{(\bo{R}^{-1} \bo{B}^\top \bo{S} \bo{x})}^\top \bo{R} \textcolor{myred}{(\bo{R}^{-1} \bo{B}^\top \bo{S} \bo{x})}
			+ 
			\bo{x}^\top (
			\bo{Q} + \bo{S} \bo{A} + \bo{A}^\top \bo{S}
			)\bo{x}
			- 
			\\
			-
			\bo{x}^\top \bo{S} \bo{B} \textcolor{myred}{(\bo{R}^{-1} \bo{B}^\top \bo{S} \bo{x})}
			- 
			\textcolor{myred}{(\bo{R}^{-1} \bo{B}^\top \bo{S} \bo{x})}^\top \bo{B}^\top \bo{S} \bo{x} = 0
			\\
		\end{align*}
		%
		\begin{align*}
				\bo{x}^\top (
				\bo{Q} + \bo{S} \bo{A} + \bo{A}^\top \bo{S}
				+
				 \bo{S} \bo{B} \bo{R}^{-1} \bo{R} \mathbf  R^{-1} \bo{B}^\top \bo{S} -
				 \\
				 - \bo{S} \bo{B} \bo{R}^{-1} \bo{B}^\top \bo{S} 
				 - \bo{S} \bo{B} \bo{R}^{-1} \bo{B}^\top \bo{S} 
				 )  \bo{x} 
				= 0
		\end{align*}
		
		Simplifying, we get: 
		
		\begin{equation}
			\bo{x}^\top (\bo{Q} + \bo{S} \bo{A} + \bo{A}^\top \bo{S}
			- \bo{S} \bo{B} \bo{R}^{-1} \bo{B}^\top \bo{S}) \bo{x} = 0
		\end{equation}
		%
		
		
	\end{flushleft}
\end{frame}



\begin{frame}{Linear Quadratic Regulator, 3}
% \framesubtitle{Algebraic Riccati}
\begin{flushleft}

The condition $\bo{x}^\top (\bo{Q} + \bo{S} \bo{A} + \bo{A}^\top \bo{S}
- \bo{S} \bo{B} \bo{R}^{-1} \bo{B}^\top \bo{S}) \bo{x} = 0$ holds for all $\bo{x}$ iff:

\begin{equation}
	\boxed{
	\bo{Q} - \bo{S} \bo{B} \bo{R}^{-1} \bo{B}^\top \bo{S} 
	+ \bo{S} \bo{A} + \bo{A}^\top \bo{S} = 0}
\end{equation}

This is the \emph{Algebraic Riccati equation}.

\end{flushleft}
\end{frame}



\begin{frame}{LQR via software}
%\framesubtitle{Numerical methods}
\begin{flushleft}

There are a number of ways to solve LQR:

\bigskip

\begin{itemize}
    \item In MATLAB there is a function \texttt{[K,S,P] = lqr(A,B,Q,R), where P=eig(A-B*K)}
    \item In Python, there is \texttt{S = scipy.linalg.solve\_continuous\_are(A,B,Q,R)}
%    \item In Drake there is a function \texttt{(K,S) = LinearQuadraticRegulator(A,B,Q,R)}
\end{itemize}

\end{flushleft}
\end{frame}



\begin{frame}{LQR and pole placement}
%	\framesubtitle{Numerical methods}
	\begin{flushleft}
		
		\begin{itemize}
			\item Pole placement \textcolor{mydarkgreen}{upsides}: allows to design exactly how fast the control error decays to zero; allows to design control error oscillations.
			
			\item Pole placement \textcolor{red}{downsides}: may require unreasonably high control gains. Easy to ask for "unreasonable" performance.
			
			\item LQR \textcolor{mydarkgreen}{upsides}: easy to produce "reasonable" control gains.
			
			\item LQR \textcolor{red}{downsides}: may produce very slow decaying control error with oscillations.
		\end{itemize}
		
		
	\end{flushleft}
\end{frame}




\begin{frame}{Discrete case}
	%	\framesubtitle{Numerical methods}
	\begin{flushleft}
		
		Consider discrete dynamics:
		
		\begin{equation}
			\bo{x}_{i+1} = \bo{A}\bo{x}_i + \bo{B}\bo{u}_i
		\end{equation}
		
		with a cost function:
		
		\begin{equation}
			J = \sum_{i=0}^\infty (\bo{x}_i\T \bo{Q} \bo{x}_i +  \bo{u}_i\T \bo{R} \bo{u}_i)
		\end{equation}
		
		Let us find the optimal control policy for this case.
		
		
	\end{flushleft}
\end{frame}



\begin{frame}{Cost-to-go, 1}
	%	\framesubtitle{Numerical methods}
	\begin{flushleft}
		
		Let us define cost-to-go as optimal cost for given initial conditions:
		%
		\begin{equation}
			V_0 = \underset{\bo{u}}{\text{min}} \sum_{i=0}^\infty (\bo{x}_i\T \bo{Q} \bo{x}_i +  \bo{u}_i\T \bo{R} \bo{u}_i)
		\end{equation}
		
		If $\bo{x}_0$, $\bo{x}_1$, $\bo{x}_2$, ... is a sequence of states that form an optimal trajectory, let us define the cost-to-go starting from each of these states as:
		%
		\begin{equation}
			V_i =  \underset{\bo{u}}{\text{min}} \sum_{k = i}^\infty (\bo{x}_k\T \bo{Q} \bo{x}_k +  \bo{u}_k\T \bo{R} \bo{u}_k)
		\end{equation}
		
		We can note that the optimal cost will take a form of a quadratic function:
		
		\begin{equation}
			V_i = \bo{x}_i\T \bo{P}_i \bo{x}_i
		\end{equation}
		
		
		
	\end{flushleft}
\end{frame}




\begin{frame}{Cost-to-go, 2}
	%	\framesubtitle{Numerical methods}
	\begin{flushleft}
		
		We can write cost-to-go as:
		
		\begin{equation}
			\label{Bellman}
			V_i(\bo{x}_i) = \underset{\bo{u}_i}{\text{min}} 
						\left(
						\bo{x}_i\T \bo{Q} \bo{x}_i +  \bo{u}_i\T \bo{R} \bo{u}_i  + V_{i+1}(\bo{x}_{i+1})  
						\right)
		\end{equation}
		%
		where $V_{i+1}(\bo{x}_{i+1})$ is the optimal cost-to-go on the next step. 
		
		\bigskip
		
		As the next step is closer to the goal, the optimal cost-to-go on the next step is both smaller than on the current step, and is contained in it.
		
		\bigskip
		
		The equation \eqref{Bellman} is called \emph{Bellman} equation.
		
	\end{flushleft}
\end{frame}



\begin{frame}{Discrete LQR, 1}
	%	\framesubtitle{Numerical methods}
	\begin{flushleft}
		
		Since $V_i = \bo{x}_i\T \bo{P}_i \bo{x}_i$ and $V_{i+1} = \bo{x}_{i+1}\T \bo{P}_{i+1} \bo{x}_{i+1}$ we can re-write Bellman equation as:
		
		\begin{equation}
						\bo{x}_i\T \bo{P}_i \bo{x}_i = \underset{\bo{u}_i}{\text{min}} 
			\left(
			\bo{x}_i\T \bo{Q} \bo{x}_i +  \bo{u}_i\T \bo{R} \bo{u}_i  + \bo{x}_{i+1}\T \bo{P}_{i+1} \bo{x}_{i+1}
			 \right)
		\end{equation}
		
		To find minimum over $\bo{u}_i$ we set partial derivative to zero:
		
		\begin{align*}
			\frac{\partial}{\partial \bo{u}_i}
			\left( 
			\bo{x}_i \T \bo{Q} \bo{x}_i  +  \bo{u}_i\T \bo{R}\bo{u}_i + 
			(\bo{A}\bo{x}_i  + \bo{B}\bo{u}_i)\T \bo{P}_{i+1} (\bo{A}\bo{x}_i  + \bo{B}\bo{u}_i) 
			\right) = 0
			\\
			2\bo{u}_i\T \bo{R}  + 
			2(\bo{A}\bo{x}_i + \bo{B}\bo{u}_i)\T \bo{P}_{i+1} \bo{B} = 0
			\\
			\bo{R}\bo{u}_i + \bo{B}\T \bo{P}_{i+1} \bo{B} \bo{u}_i + \bo{B}\T \bo{P}_{i+1} \bo{A}\bo{x}_i  = 0
			\\
			(\bo{R} + \bo{B}\T \bo{P}_{i+1} \bo{B}) \bo{u}_i = -\bo{B}\T \bo{P}_{i+1} \bo{A}\bo{x}_i 
			\\
			\bo{u}_i = -(\bo{R} + \bo{B}\T \bo{P}_{i+1} \bo{B})^{-1} \bo{B}\T \bo{P}_{i+1} \bo{A}\bo{x}_i 
		\end{align*}

		
	\end{flushleft}
\end{frame}






\begin{frame}{Back-propagation, 1}
	%	\framesubtitle{Numerical methods}
	\begin{flushleft}
		
		Let us define $\bo{M} = (\textcolor{myblue}{\bo{R} + \bo{B}\T \bo{P}_{i+1} \bo{B}})^{-1}$ and $\bo{N} =\textcolor{mydarkgreen}{\bo{B}\T \bo{P}_{i+1} \bo{A}}$ we can re-write the control law:
		%
		\begin{equation}
			\bo{u}_i = -\bo{M} \bo{N} \bo{x}_i
		\end{equation}
		
		We can substitute the control law into the Bellman eq.:
	%
		\begin{equation*}
			\bo{x}_i\T \bo{P}_i \bo{x}_i
			= \underset{\bo{u}}{\text{min}} 
			(\bo{x}_i\T \bo{Q} \bo{x}_i +  \textcolor{myred}{\bo{u}_i}\T \bo{R} \textcolor{myred}{\bo{u}_i}  + 
			(\bo{A}\bo{x}_i + \bo{B}\textcolor{myred}{\bo{u}_i})\T \bo{P}_{i+1} (\bo{A}\bo{x}_i + \bo{B}\textcolor{myred}{\bo{u}_i}) 
			)
		\end{equation*}
		%
		\begin{align*}
		\bo{x}_i\T \bo{P}_i \bo{x}_i
		= 
		\bo{x}_i\T \bo{Q} \bo{x}_i +  \textcolor{myred}{\bo{x}_i\T \bo{N}\T \bo{M}} \bo{R} \textcolor{myred}{\bo{M} \bo{N} \bo{x}_i}  + 
		\\
		+
		(\bo{A}\bo{x}_i - \bo{B}\textcolor{myred}{\bo{M} \bo{N} \bo{x}_i})\T \bo{P}_{i+1} (\bo{A}\bo{x}_i - \bo{B}\textcolor{myred}{\bo{M} \bo{N} \bo{x}_i}) 
		\end{align*}
		%
		\begin{align*}
		\bo{P}_i 
		= 
		\bo{Q} +  \bo{N}\T \bo{M}\textcolor{myblue}{ \bo{R}} \bo{M} \bo{N} + \bo{A}\T \bo{P}_{i+1} \bo{A} - 
		\textcolor{mydarkgreen}{\bo{A}\T \bo{P}_{i+1} \bo{B}} \bo{M} \bo{N} - \\
		 \bo{N}\T  \bo{M} \textcolor{mydarkgreen}{\bo{B}\T  \bo{P}_{i+1} \bo{A}}  +
		 \bo{N}\T  \bo{M}\textcolor{myblue}{ \bo{B}\T  \bo{P}_{i+1} \bo{B}} \bo{M} \bo{N} 
		\end{align*}
		%
		\begin{align*}\hspace*{-0.8cm} %\scriptstyle
		\bo{P}_i 
		= 
		\bo{Q}  + \bo{A}\T \bo{P}_{i+1} \bo{A}  +  \bo{N}\T \bo{M}\textcolor{myblue}{ (\bo{R} + \bo{B}\T  \bo{P}_{i+1} \bo{B})} \bo{M} \bo{N}
		-\bo{N}\T \bo{M} \bo{N} 
		-\bo{N}\T  \bo{M} \bo{N}  
		\end{align*}
		%
		\begin{align*}
		\bo{P}_i = 	\bo{Q} + \bo{A}\T \bo{P}_{i+1} \bo{A} - \bo{N}\T  \bo{M} \bo{N}  
		\end{align*}
		
		
		
	\end{flushleft}
\end{frame}




\begin{frame}{Back-propagation, 2}
	%	\framesubtitle{Numerical methods}
	\begin{flushleft}
		
		From $\bo{P}_i = 	\bo{Q} + \bo{A}\T \bo{P}_{i+1} \bo{A} - \textcolor{mydarkgreen}{\bo{N}}\T  \textcolor{myblue}{\bo{M}} \textcolor{mydarkgreen}{\bo{N}}  $ we obtain the final result:
		
		\begin{align*}
			\bo{P}_i = 	\bo{Q} + \bo{A}\T \bo{P}_{i+1} \bo{A} - 
			\textcolor{mydarkgreen}{\bo{A}\T\bo{P}_{i+1}\bo{B}}  \textcolor{myblue}{(\bo{R} + \bo{B}\T \bo{P}_{i+1} \bo{B})^{-1} } \textcolor{mydarkgreen}{\bo{B}\T \bo{P}_{i+1} \bo{A}}
		\end{align*}
		
		This equation can be used to compute $\bo{P}_i$ from known $\bo{P}_{i+1}$.
		
	\end{flushleft}
\end{frame}


\begin{frame}{Further reading}
	%	\framesubtitle{Numerical methods}
	\begin{flushleft}
		
\begin{itemize}
	\item \bref{http://underactuated.mit.edu/lqr.html}{Underactuated robotics. Linear Quadratic Regulators}.
	
	\item \bref{https://stanford.edu/class/ee363/lectures/dlqr.pdf}{Discrete LQR. Stanford, EE363}.
	
	\item \bref{https://web.stanford.edu/class/ee363/lectures/dlqr-ss.pdf}{Discrete LQR (infinite horizon). Stanford, EE363}.
	
	\item \bref{https://ocw.mit.edu/courses/14-451-dynamic-optimization-methods-with-applications-fall-2009/7db0775fbdf792afff92a24b8456fc85\_MIT14\_451F09\_lec10.pdf}{MIT 14.451 Lecture Notes, 10, Guido Lorenzoni}
\end{itemize}
		
		
	\end{flushleft}
\end{frame}


\myqrframe


\begin{frame}
	\centerline{\huge Appendix A: An illustration of HJB}
\end{frame}


\begin{frame}{Optimality, definitions}
	%\framesubtitle{}
	\begin{flushleft}
		
		Consider the additive cost $J (\bo{x}_0, \pi (\bo{x})) = \int_0^\infty g(\bo{x}, \bo{u}) dt$, where $\bo{u} = \pi (\bo{x})$ is a control policy. The function $g(\bo{x}, \bo{u}) \geq 0$ can be interpreted as a rate of change of cost.
		
		\bigskip
		
		Let $\pi^*(\bo{x})$ be the optimal control policy. Applying the optimal policy to the dynamics $\dot{\bo{x}} = f(\bo{x}, \bo{u})$ we obtain optimal dynamics:
		%
		\begin{equation}
			\dot{\bo{x}} = f^*(\bo{x}) = f(\bo{x}, \pi^*(\bo{x}))
		\end{equation}
		
		Given initial conditions $\bo{x}_0 = \bo{z}$ we generate an optimal trajectory $\bo{x}^* = \bo{x}^*(t, \bo{z})$. Given optimal trajectory and optimal control policy we find optimal cost:
		%
		\begin{equation}
			J^*(\bo{z}) = J(\bo{z}, \pi^* (\bo{x}))
		\end{equation}
		
		Equivalently, we find optimal instantenious cost:
		%
		\begin{equation}
			g^*(\bo{x}) = g(\bo{x}, \pi^* (\bo{x}))
		\end{equation}
		
		
	\end{flushleft}
\end{frame}



\begin{frame}{Incurred cost}
	%\framesubtitle{}
	\begin{flushleft}
		
		Since optimal cost depends on initial conditions only, we can find a function $J^* = J^*(\bo{z})$ defined over $\R^n$.
		
		\bigskip
		
		Lets us consider a trajectory $\bo{x}^* = \bo{x}^*(t, \bo{z})$ and sequence of points on this trajectory $\bo{x}_0$, $\bo{x}_1$, $\bo{x}_2$, etc. associated with the time stamps $t_0$, $t_1$, $t_2$, etc. We can define incurred cost (incurred while moving from the initial state $\bo{z} = \bo{x}_0$ to the given point) for each of these points $S_0$, $S_1$, $S_2$, etc. as:
		
		\begin{equation}
			S_i = \int_0^{t_i} g^*(\bo{x}) dt
		\end{equation}
		
		Since $g^*(\bo{x}) \geq 0$, we observe that $S_0 \leq S_1 \leq S_2 \leq ...$. Moving along a trajectory $\bo{x}^*(t, \bo{z})$ we incur monotonically increasing cost. We can describe it as a time function $S(t)$. The rate of increace of this function is given by instantenious cost $g^*(\bo{x})$.
		
		
	\end{flushleft}
\end{frame}



\begin{frame}{Cost-to-go}
	%\framesubtitle{}
	\begin{flushleft}
		
		We know that the optimal cost from the point $\bo{z}$ is given as $J^*(\bo{z})$. For each sequential point on the trajectory we can define \emph{cost-to-go} $V_i$ as a difference between the optimal cost and the incurred cost:
		
		\begin{equation}
			V_i = J^*(\bo{z}) - S_i
		\end{equation}
		
		For a given trajectory, we can describe cost-to-go as a time function $V(t) = J^*(\bo{z}) - S(t)$. Where as $S(t)$ is monotonically increasing, the function $V(t)$ is monotonically decreasing, with a rate of change given as $-g^*(\bo{x})$.
		
		\bigskip
		
		Note that the cost-to-go can be equivalently found as:
		
		\begin{equation}
			V(t) = J^*(\bo{x}^*(t))
		\end{equation}
		%
		since the optimal cost we incur by starting from the point $\bo{x}_i$ is equivalent to the cost "have left to incur" when we reach $\bo{x}_i$ from $\bo{x}_0$.
		
	\end{flushleft}
\end{frame}


\begin{frame}{Optimality}
	%\framesubtitle{}
	\begin{flushleft}
		
		With that, we can make an observation: for an optimal policy we see the rate of change of the cost-to-go function equal to $-g^*(\bo{x})$. But this rate of change can be found by taking a derivative of $J^*(\bo{x})$ with respect to the vector field $\dot{\bo{x}} = f^*(\bo{x})$:
		
		\begin{equation}
			-g^*(\bo{x}) = \frac{\partial J^*}{\partial \bo{x}} f^*(\bo{x})
		\end{equation}
		
		For sub-optimal control policies, the incurred cost will outpace the decrease of the cost-to-go:
		
		\begin{equation}
			g(\bo{x}, \bo{u}) + \frac{\partial J^*}{\partial \bo{x}} f(\bo{x}, \bo{u}) \geq 0
		\end{equation}
		
		Optimal policy recovers the sought equality:
		
		\begin{equation}
			\underset{\bo{u}}{\min} \ 
			\left[ 
			g(\bo{x}, \bo{u}) + 
			\frac{\partial J^*}{\partial \bo{x}} \bo{f} (\bo{x}, \bo{u}) 
			\right] = 0
		\end{equation}
		
	\end{flushleft}
\end{frame}





\end{document}
